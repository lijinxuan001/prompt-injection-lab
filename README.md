
# ðŸ§  Prompt Injection Lab

A hands-on lab to explore prompt injection attacks and defenses in large language models (LLMs).  
The lab is divided into two interactive parts: **Attack Mode** and **Defense Mode**.

---

## ðŸŽ¯ Modes

### Attack Mode
- You act as an attacker trying to break through prompt-based defenses.
- Each level increases in difficulty with stronger model protections.

### Defense Mode
- You act as the LLM system designer.
- Your goal is to block adversarial prompts using strategies like:
  - Input filtering
  - Output filtering
  - Behavioral detection
  - Multi-LLM validation (future level)
- Includes "Evolution Defense" mode: iteratively improve until an attack is blocked.

---

## ðŸš€ Getting Started

1. Make sure [Ollama](https://ollama.com) is installed and a model is running, e.g.:
```bash
ollama run llama3
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the lab:
```bash
python main.py
```

---

## ðŸ“‚ Project Structure

```
prompt-injection-lab-final/
â”œâ”€â”€ attacker_mode/        # Attack side logic & levels
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ play.py
â”‚   â””â”€â”€ levels/
â”‚       â””â”€â”€ level_1.json
â”œâ”€â”€ defender_mode/        # Defense side logic & scenarios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ evolve.py
â”‚   â””â”€â”€ scenarios/
â”‚       â””â”€â”€ scenario_1.json
â”œâ”€â”€ engine/               # Shared model & scoring engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â””â”€â”€ score.py
â”œâ”€â”€ main.py               # Entry point
â””â”€â”€ requirements.txt      # Dependencies
```
